---
title: "Lab 3 Demo"
author: "Mateo Robbins"
date: "2023-01-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rsample)
library(skimr)
library(glmnet)
```

## Data Wrangling and Exploration
```{r data}
#load and inspect the data
dat <- AmesHousing::make_ames()

```

##Train a model
```{r intial_split}
# Data splitting with {rsample} 
set.seed(123) #set a seed for reproducibility
split <- initial_split(dat)

ames_train <-  training(split)
ames_test  <-  testing(split)
```

```{r model_data}
#Create training feature matrices using model.matrix() (auto encoding of categorical variables)

X <- model.matrix(Sale_Price ~ ., data = ames_train)[,-1]

# transform y with log() transformation
Y <- log(ames_train$Sale_Price)

```

```{r glmnet}
#fit a ridge model, passing X,Y,alpha to glmnet()
ridge <- glmnet(
  x = X, # predictors
  y = Y, # outcome variable (sale price)
  alpha = 0 # 0 = ridge, 1 = lasso, anything between 0 and 1 = elastic net
)

#plot() the glmnet model object

plot(ridge, xvar = "lambda")  
```

```{r}
# lambdas applied to penalty parameter.  Examine the first few
ridge$lambda %>% 
  head()


# small lambda results in large coefficients
coef(ridge)[c("Latitude", "Overall_QualVery_Excellent"), 100]
# based on lambdas, location seems to be more important than quality

# what about for small coefficients?
coef(ridge)[c("Latitude", "Overall_QualVery_Excellent"), 1]
# increased lambda by 100 steps, we see veryyyyyy small coefficients

# **negative relationship b/w lambda and coefficientes
  
```
How much improvement to our loss function as lambda changes?

##Tuning
```{r cv.glmnet}
# Apply CV ridge regression to Ames data.  Same arguments as before to glmnet()
# defaults to 10 folds
ridge <- cv.glmnet(
  x = X,
  y = Y,
  alpha = 0
)

# Apply CV lasso regression to Ames data
lasso <- cv.glmnet(
  x = X,
  y = Y,
  alpha = 1
)
  
# cross-validation is being used to see how well model will perform on unseen data

# plot results
par(mfrow = c(1, 2))
plot(ridge, main = "Ridge penalty\n\n")
plot(lasso, main = "Lasso penalty\n\n")
```

- first dotted line shows optimal lambda in terms of MSE (where minimum MSE occurs)
- second dotted line is 1 standard error above minimum point which increases the parsimony (aka fewer variables) of the model

10-fold CV MSE for a ridge and lasso model. What's the "rule of 1 standard error"?

In both models we see a slight improvement in the MSE as our penalty log(Î») gets larger, suggesting that a regular OLS model likely overfits the training data. But as we constrain it further (i.e., continue to increase the penalty), our MSE starts to increase. 

Let's examine the important parameter values apparent in the plots.
```{r}
# Ridge model
# minimum MSE
min(ridge$cvm) # 0.020

# lambda for this min MSE
ridge$lambda.min # 0.14

# 1-SE rule
ridge$cvm[ridge$lambda == ridge$lambda.1se] # 0.023

# lambda for this MSE
ridge$lambda.1se # 0.56

# Lasso model

# minimum MSE
min(lasso$cvm) # 0.021

# lambda for this min MSE
lasso$lambda.min # 0.003

# 1-SE rule
lasso$cvm[lasso$lambda == lasso$lambda.1se] # 0.026

# lambda for this MSE
lasso$lambda.1se # 0.013

# No. of coef | 1-SE MSE
lasso$nzero[lasso$lambda == lasso$lambda.1se]
# significantly reduced number of predictor variables in model by using lambda at 1 standard error from minimum MSE (almost halved)
```

```{r}
# Ridge model
ridge_min <- glment(
  
)

# Lasso model
lasso_min


par(mfrow = c(1, 2))
# plot ridge model
plot(ridge_min, xvar = "lambda", main = "Ridge penalty\n\n")
abline(v = log(ridge$lambda.min), col = "red", lty = "dashed")
abline(v = log(ridge$lambda.1se), col = "blue", lty = "dashed")

# plot lasso model
plot(lasso_min, xvar = "lambda", main = "Lasso penalty\n\n")
abline(v = log(lasso$lambda.min), col = "red", lty = "dashed")
abline(v = log(lasso$lambda.1se), col = "blue", lty = "dashed")
```

